{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid, ZINC, QM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import NormalizeFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())   ## This should be fine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Cora',\n",
       " 'root': 'data/Planetoid',\n",
       " 'transform': NormalizeFeatures(),\n",
       " 'pre_transform': None,\n",
       " 'pre_filter': None,\n",
       " '_indices': None,\n",
       " 'data': Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708]),\n",
       " 'slices': None,\n",
       " '_data_list': None,\n",
       " 'split': 'public'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  19],\n",
       "        [  81],\n",
       "        [ 146],\n",
       "        [ 315],\n",
       "        [ 774],\n",
       "        [ 877],\n",
       "        [1194],\n",
       "        [1247],\n",
       "        [1274]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.x[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "dataset.data.x[0][[19, 81, 146]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets look at the ZINC and QM9 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://data.pyg.org/datasets/qm9_v3.zip\n",
      "Extracting data/QM9/raw/qm9_v3.zip\n",
      "Processing...\n",
      "Using a pre-processed version of the dataset. Please install 'rdkit' to alternatively process the raw data.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "zinc_dataset = ZINC(root='data/ZINC', transform=NormalizeFeatures()) \n",
    "QM9_dataset = QM9(root='data/QM9', transform=NormalizeFeatures()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's the OpenGraphBenchmark - Arxiv citation dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:42<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/arxiv/arxiv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading necessary files...\n",
      "This might take a while.\n",
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26886.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4013.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "arxiv_dataset = PygNodePropPredDataset(name = \"ogbn-arxiv\", root = 'data/arxiv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ogbn-arxiv',\n",
       " 'dir_name': 'ogbn_arxiv',\n",
       " 'original_root': 'data/arxiv',\n",
       " 'root': 'data/arxiv/ogbn_arxiv',\n",
       " 'meta_info': num tasks                                                                1\n",
       " num classes                                                             40\n",
       " eval metric                                                            acc\n",
       " task type                                        multiclass classification\n",
       " download_name                                                        arxiv\n",
       " version                                                                  1\n",
       " url                      http://snap.stanford.edu/ogb/data/nodeproppred...\n",
       " add_inverse_edge                                                     False\n",
       " has_node_attr                                                         True\n",
       " has_edge_attr                                                        False\n",
       " split                                                                 time\n",
       " additional node files                                            node_year\n",
       " additional edge files                                                 None\n",
       " is hetero                                                            False\n",
       " binary                                                               False\n",
       " Name: ogbn-arxiv, dtype: object,\n",
       " 'download_name': 'arxiv',\n",
       " 'num_tasks': 1,\n",
       " 'task_type': 'multiclass classification',\n",
       " 'eval_metric': 'acc',\n",
       " '__num_classes__': 40,\n",
       " 'is_hetero': False,\n",
       " 'binary': False,\n",
       " 'transform': None,\n",
       " 'pre_transform': None,\n",
       " 'pre_filter': None,\n",
       " '_indices': None,\n",
       " 'data': Data(edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343, 1]),\n",
       " 'slices': None,\n",
       " '_data_list': None}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_dataset.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train, 90941', 'valid, 29799', 'test, 48603']\n"
     ]
    }
   ],
   "source": [
    "arxiv_dataset.get_idx_split()\n",
    "print([f\"{key}, {len(a)}\" for key, a in arxiv_dataset.get_idx_split().items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea63fb173e44a359126a19be1eea0c8e6496573267ac298b821cc3c23918084f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('bottleneck': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
